# ML_Class4
## Batch Normalization: ç”¨CNNä¾†åšå½±åƒè™•ç†çš„æ™‚å€™ï¼Œbatch nomalizationå¾€å¾€å¯ä»¥å¸¶ä¾†å¾ˆå¥½çš„å¹«åŠ©ï¼
### å¦‚æœerror surfaceå¾ˆå´å¶‡çš„æ™‚å€™ï¼Œèƒ½ä¸èƒ½ç›´æ¥æŠŠå±±éŸå¹³ï¼Ÿ å¯ä»¥ç”¨changing ladscapeè®“ä»–è®Šå¾—æ¯”è¼ƒå¥½trainï¼
1. åœ¨modelä¸­ï¼Œç•¶inputçš„featureï¼Œå…¶æ¯ä¸€å€‹dimensionçš„scaleå€¼å·®è·å¾ˆå¤§æ™‚ï¼Œå°±å¯èƒ½ç”¢ç”Ÿä¸åŒæ–¹å‘æ–œç‡ã€å¡åº¦éå¸¸ä¸åŒçš„error surface

   æ˜¯å¦æœ‰å¯èƒ½çµ¦ä¸åŒfeatureçš„dimensionï¼Œè®“ä»–æœ‰åŒæ¨£çš„æ•¸å€¼ç¯„åœï¼Œå°±èƒ½è£½é€ æ¯”è¼ƒå¥½çš„error surfaceï¼Œè®“ä»–æ¯”å«å¥½trainingï¼

2. ä»¥ä¸Šå•é¡Œå¯é€éfeature normalization

   åœ¨activation functionä¹‹å‰æˆ–ä¹‹å¾Œåšnormalizationçš„å·®ç•°å…¶å¯¦ä¸å¤§ï¼

3. if batch size = 64, the large networkæœƒæŠŠ64æ¯”dataè®€é€²å»

   å†ç®—é€™64ç­†dataçš„miuè·Ÿsigmaï¼Œå»å°é€™64ç­†dataå»åšnormalization

   -> batch normalization (é©ç”¨æ–¼batch sizeæ¯”è¼ƒå¤§çš„æ™‚å€™)

4. ç‚ºä»€éº¼è¦åŠ ä¸ŠÎ²è·ŸÎ³å‘¢ï¼Ÿ

   å› ç‚ºåšå®Œnormalizationä¹‹å¾Œçš„hidden layer outputå¹³å‡æœƒï¼0ï¼Œæœ‰å¯èƒ½æœƒå¸¶ä¾†ä¸€äº›é™åˆ¶

   æ‰€ä»¥æœƒè‡ªå·±å»learn Î²è·ŸÎ³çš„å€¼ï¼Œä¾†èª¿æ•´z hatçš„åˆ†ä½ˆï¼Œè€ŒÎ³çš„åˆå§‹å€¼ç‚º1-vectorï¼ŒÎ²çš„åˆå§‹å€¼ç‚º0-vectorï¼Œæ¯å€‹dimensionæ‰ä¸æœƒè½å·®å¤ªå¤§

5. batch normalizatin-testing

   åœ¨æ¸¬è©¦çš„æ™‚å€™ï¼ŒWe do not always have batch at testing stageï¼Œæ‰€ä»¥ç„¡æ³•ç®—å‡ºğè·Ÿğˆ.

   å› æ­¤æ”¹ç®—the moving average of ğ and ğˆ of the batches during training.
   
6. trainingå¯èƒ½æœƒé‡åˆ°çš„å•é¡Œï¼šInternal Covariate Shift?
   
   batch normalizationæœƒæœ‰å¹«åŠ©ä¸ä¸€å®šæ˜¯å› ç‚ºè§£æ±ºäº†internal covariate shift



