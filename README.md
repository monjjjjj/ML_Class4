# ML_Class4
## Batch Normalization: ç”¨CNNä¾†åšå½±åƒè™•ç†çš„æ™‚å€™ï¼Œbatch nomalizationå¾€å¾€å¯ä»¥å¸¶ä¾†å¾ˆå¥½çš„å¹«åŠ©ï¼
### å¦‚æœerror surfaceå¾ˆå´å¶‡çš„æ™‚å€™ï¼Œèƒ½ä¸èƒ½ç›´æ¥æŠŠå±±éŸå¹³ï¼Ÿ å¯ä»¥ç”¨changing ladscapeè®“ä»–è®Šå¾—æ¯”è¼ƒå¥½trainï¼
1. åœ¨modelä¸­ï¼Œç•¶inputçš„featureï¼Œå…¶æ¯ä¸€å€‹dimensionçš„scaleå€¼å·®è·å¾ˆå¤§æ™‚ï¼Œå°±å¯èƒ½ç”¢ç”Ÿä¸åŒæ–¹å‘æ–œç‡ã€å¡åº¦éå¸¸ä¸åŒçš„error surface

   æ˜¯å¦æœ‰å¯èƒ½çµ¦ä¸åŒfeatureçš„dimensionï¼Œè®“ä»–æœ‰åŒæ¨£çš„æ•¸å€¼ç¯„åœï¼Œå°±èƒ½è£½é€ æ¯”è¼ƒå¥½çš„error surfaceï¼Œè®“ä»–æ¯”è¼ƒå¥½trainingï¼

2. ä»¥ä¸Šå•é¡Œå¯é€éfeature normalization

   åœ¨activation functionä¹‹å‰æˆ–ä¹‹å¾Œåšnormalizationçš„å·®ç•°å…¶å¯¦ä¸å¤§ï¼

3. if batch size = 64, the large networkæœƒæŠŠ64ç­†dataè®€é€²å»

   å†ç®—é€™64ç­†dataçš„miuè·Ÿsigmaï¼Œå»å°é€™64ç­†dataå»åšnormalization

   -> batch normalization (é©ç”¨æ–¼batch sizeæ¯”è¼ƒå¤§çš„æ™‚å€™)

4. ç‚ºä»€éº¼è¦åŠ ä¸ŠÎ²è·ŸÎ³å‘¢ï¼Ÿ

   å› ç‚ºåšå®Œnormalizationä¹‹å¾Œçš„hidden layer outputå¹³å‡æœƒï¼0ï¼Œæœ‰å¯èƒ½æœƒå¸¶ä¾†ä¸€äº›é™åˆ¶

   æ‰€ä»¥æœƒè‡ªå·±å»learn Î²è·ŸÎ³çš„å€¼ï¼Œä¾†èª¿æ•´z hatçš„åˆ†ä½ˆï¼Œè€ŒÎ³çš„åˆå§‹å€¼ç‚º1-vectorï¼ŒÎ²çš„åˆå§‹å€¼ç‚º0-vectorï¼Œæ¯å€‹dimensionæ‰ä¸æœƒè½å·®å¤ªå¤§

5. batch normalizatin-testing

   åœ¨æ¸¬è©¦çš„æ™‚å€™ï¼ŒWe do not always have batch at testing stageï¼Œæ‰€ä»¥ç„¡æ³•ç®—å‡ºğè·Ÿğˆ.

   å› æ­¤æ”¹ç®—the moving average of ğ and ğˆ of the batches during training.
   
6. trainingå¯èƒ½æœƒé‡åˆ°çš„å•é¡Œï¼šInternal Covariate Shift?
   
   batch normalizationæœƒæœ‰å¹«åŠ©ä¸ä¸€å®šæ˜¯å› ç‚ºè§£æ±ºäº†internal covariate shift
   
## Transformer: seq2seq
1. multi-class classification: ä¸åªä¸€ç¨®classï¼Œæ©Ÿå™¨è¦å¾å¤šç¨®classä¸­é¸å‡ºä¸€å€‹ä¾†

   multi-label classification: åŒä¸€å€‹æ±è¥¿å¯ä»¥å±¬æ–¼å¤šå€‹class
   
2. seq2seq is a powerful model
   
   encoder: inputä¸€æ’å‘é‡ï¼Œoutputä¸€æ’å‘é‡ï¼ˆè¼¸å…¥ä¸€å€‹vector seqï¼Œè¼¸å‡ºä¸€å€‹vector seqï¼‰
   
   åœ¨transformerè£¡çš„encoderç”¨çš„å°±æ˜¯self-attention

3. residual connection 

   residual vector = input vector + output vector

4. Decoder-Autoregressive
   
   decoderæœƒæŠŠè‡ªå·±çš„è¼¸å‡ºï¼Œç•¶ä½œä¸‹ä¸€å€‹éšæ®µçš„è¼¸å…¥ -> æ˜¯å¦æœƒç”¢ç”Ÿerror propagationçš„å•é¡Œå‘¢ï¼Ÿ

   Masked self-attention
   
       èˆ‰ä¾‹ï¼šb2åªè€ƒæ…®a1è·Ÿa2è€Œä¸è€ƒæ…®a3è·Ÿa4ï¼Œå› ç‚ºdecoderçš„è¼¸å‡ºæ˜¯ä¸€å€‹ä¸€å€‹ç”¢ç”Ÿçš„ï¼
       
5. AT -> ä¸€æ¬¡ç”¢ç”Ÿä¸€å€‹å­—

   NAT -> ä¸€æ¬¡ç”¢ç”Ÿä¸€å€‹å¥å­

6. encoderè·Ÿdecoderä¹‹é–“å¦‚ä½•å‚³éè³‡è¨Šï¼Ÿ

   è—‰ç”±cross attention(k&vä¾†è‡ªencoder, qä¾†è‡ªdecoder)
   
7. Teacher Forcing: using the ground truth as input of decoder (ç•¶è¨“ç·´çš„æ™‚å€™ç”¨ground truthç•¶ä½œinput? ä½†testingçš„æ™‚å€™å°±ç„¡æ³•ç”¨ground truthä¾†ç•¶inputäº†ï¼Ÿ)
8. Training tips

    (1) Copy mechanism: å¾inputçš„è³‡è¨Šè¤‡è£½ä¸€äº›æ±è¥¿å‡ºä¾†åˆ°output
    
    (2) Guided Attention: è¦æ±‚æ©Ÿå™¨åœ¨åšattentionçš„æ™‚å€™æ˜¯æœ‰å›ºå®šæ–¹å¼çš„ï¼
    
    (3) Beam Search




